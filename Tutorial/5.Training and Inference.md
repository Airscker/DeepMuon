# Training and Inference

## Single GPU Training

### Command

```bash
Dmuon_train --config /home/dachuang2022/Yufeng/DeepMuon/config/Hailing/MLP3_3D.py
```
Parameters:
- `--config`: Absolute path of the configuration file
## Distributed Data Parallel Training

### Command

```bash
Dmuon_train --dist --gpus 0 1 2 3 -p 22921 -t dist_train --config /home/dachuang2022/Yufeng/DeepMuon/config/Hailing/MLP3_3D.py
```
Parameters:
- `-g`,`--gpus`: The ID of GPUs to be used during distributed training

- `-p`,`--port`: Data exchange port during DDP

- `-t`,`--train`: Customized/Default training pipeline, can be ignored if you have no customized demands

- `-c`,`--config`: Absolute path of the configuration file

## Results of Training
- TensorBoard Log: `{work_dir}/LOG`
- Epoch Information and other console ouputs: `{work_dir}/log.log`
- (Best Performance) Checkpoints: `{work_dir}/*.pth`
  
## Inference
### Command
```bash
Dmuon_infer --config /home/dachuang2022/Yufeng/DeepMuon/config/Hailing/Vit.py --ana True --thres 0.01
```
Parameters:
- `--config`: Absolute path of the configuration file
- `--ana`: Whether to analysis the model performance on test dataset given by the configuration file
- `--thres`: The threshold value used to get the loss distribution of resampled dataset whose loss value is under the threshold.

This operation will get the performance of model on test dataset, and plot the loss distribution of tested data samples.
And every single data sample's loss value, predicted value, real value, data ID  hash map: ID-[data,pred,real,loss] will be saved into the folder `{work_dir}/infer/inference_res.pkl`.
At the same time, all data samples' performance will be saved as `{work_fir}/infer/inference.log`

If parameter `--ana==True`, then the curve of learning rate and the distribution of the loss will be saved into `{work_dir}/ana`.
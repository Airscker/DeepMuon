# Configuration

## Introduction

Here we introduce another core component of **DeepMuon**: Configuration Files.

Configuration files are used to eliminate the modification of `train.py`,`dist_train.py`, which are core of training, we can make sure every researcher is smart enough to understand the logic of those core files, we want to provide a simple and direct experience of customizing training configurations, the DeepMuon training frame will automatically recognize the configurations specified within configuration files and apply them into training.

Configuration files are recommended to be stored in the folder `config`, before you create your configuration files, you must **STRICTLY** follow these rules:

> 1. **One Project, One Folder**
>
>    Here we know that there are several different projects are waiting for us. And during exploration of one project we have to try different configurations several times, so we need to make sure our management of projects and configurations of one project is tidy, direct, clear and even beautiful. Just as what I said before, researches are not only challenges to our intelligence but also to our taste of beauty and tidy.
>
> 3. **The edition of configuration files must follow the regulations given in the section `Configuration File Regulations`**
>
> 5. **DO NOT ADD** any executable console scripts into Dataset file, for instance: `print()`,`plt.show()`... all kinds of executable console scripts all forbidden in the file.

## Configuration File Regulations

1. **All these keywords must be presented in a configuration file:**

   - model
   - train_dataset
   - test_dataset
   - work_config
   - checkpoint_config
   - loss_fn
   - hyperpara
   - <font color="red">lr_config(will be deprecated in the future version and we recommand to use `optimizer` and `` intstead)</font>
   - evaluation
   - optimizer
   - scheduler
   - gpu_config

2. **Regulations of keywords:**

   - Specify the model to be trained: 
   
     `model=dict(filepath='',backbone='MLP3_3D_Direc',params=dict())`

     - filepath: the path of your model, if your model is stored in `DeepMuon.models` you can omit this parameter or just let `filepath=''`
     - backbone: specify the model name, model will be picked up in the folder `models`
     - params: the initialization parameters used to create the model object, such as `params=dict(dropout_rate=0.1)`,if your model don't have any initialization parameters, just omit it or let `params=dict()`
   
   - Specify the training/testing dataset to be used to load the data, all dataset are stored in `dataset`: 
   
       `train_dataset=dict(filepath='',backbone='HailingDataset_Direct',datapath='/home/dachuang2022/Yufeng/Hailing-Muon/data/1TeV/Hailing_1TeV_train_data.pkl')`
   
       `train_dataset=dict(filepath='',backbone='HailingDataset_Direct',params=dict(datapath='/home/dachuang2022/Yufeng/Hailing-Muon/data/1TeV/Hailing_1TeV_train_data.pkl'))`
   
       - filepath: the path of your dataset, if your model is stored in `DeepMuon.dataset` you can omit this parameter or just let `filepath=''`
       - backbone: specify the name of Dataset class, Dataset will be loaded from folder `dataset`
       - datapath: specify the path of data, absolute path needed (**deprecated**)
       - params: the initialization parameters used to create the dataset object, such as `params=dict(datapath='...',samples='...')`,if your dataset don't have any initialization parameters, just omit it or let `params=dict()`
   
   - Specify the work_dir to save the training log and checkpoints
       `work_config=dict(work_dir='/home/dachuang2022/Yufeng/Hailing-Muon/work_dir/1TeV/MLP3_3D',logfile='log.log')`
       
       - work_dir: the folder used to store the training logfile, tensorboard event foler `LOG`, and checkpoints. **Absolute path needed**
       - logfile: the name of the training logfile
       
   - Specify the checkpoint configuration
       `checkpoint_config=dict(load_from='',resume_from='',save_inter=10)`
       
       - load_from: the path of the pretrained `.pth` file to be loaded, model will be trained from epoch 0
       - resume_from: the path of pretrained `.pth` file to be used to resume the model training
       - save_inter: specify the checkpoint saving frequency
       
   - Specify the customized loss function to be used, if no customized loss function specified, nn.MSELoss() will be used
     
       If `loss_fn=None` specified, `nn.MSELoss()` will be used to train the model, otherwise `loss_fn=dict(filepath='',backbone='',params=dict())`
       
       - filepath: the path of your loss function, if your loss_fn is stored in `DeepMuon.models.Airloss` you can omit this parameter or just let `filepath=''`
       - backbone: the name of the loss function
       - params: the initialization parameters used to create the loss function object, such as `params=dict(L1_ratio='...',L2_ratio='...')`,if your loss function don't have any initialization parameters, just omit it or let `params=dict()`
       
   - Specify the Hyperparameters to be used
       `hyperpara=dict(epochs=10,batch_size=400,inputshape=[1,10,10,40,3])`
       
       - epochs: the training epochs
       - batch_size: the training batch_size
       - inputshape: the shape of the model input data, first element is the batch_size(here is 1), and the left elements are actual data shape
       
   - Specify the lr as well as its config, the lr will be optimized using `torch.optim.lr_scheduler.ReduceLROnPlateau()` and `optimizer` will be set as `torch.optim.SGD()`
       `lr_config=dict(init=0.0005,patience=100)`
       
       - init: the initial learning rate
       - patience: the patience epochs used to judge the learning rate dacay status

   - Specify the evaluation configuration, if parameter `evaluation` is not specified, the model will be trained to minimize the loss value.
   
       `evaluation = dict(interval=1, metrics=['f1_score', 'confusion_matrix', 'every_class_accuracy', 'top_k_accuracy'],sota_target=dict(mode='max', target='f1_score'))`

       - interval: the interval of evaluation, if not specified, it will be set as 1.
       - metrics: the list of evalution methods defined under the folder `loss_fn`.
       - sota_target: set the optimize mode and optimize target
         - mode: min/max
         - target: target to be minimized/maximized,should be included in `metrics`.
   
   - Specify the optimizer configuration, if parameter `filepath` is not specified, the name of `optimizer` is the same as **PyTorch** provided. 
   
       `optimizer = dict(filepath='',backbone='SGD', params=dict(lr=0.0001, momentum=0.9, nesterov=True))`
   
       - filepath: the file path of the customized optimizer, can be omitted if we don't use customized optimizer
       - backbone: the name of the optimizer
       - params: the parameters used to initalize optimizer
   
   - Specify the scheduler configuration, if parameter `filepath` is not specified, the name of `` is the same as **PyTorch** provided.
   
       `scheduler = dict(filepath='',backbone='CosineAnnealingLR', params=dict(T_max=10))`
   
       - filepath: the file path of the customized scheduler, can be omitted if we don't use customized scheduler
       - backbone: the name of the scheduler
       - params: the parameters used to initalize scheduler
   
   - Specify the GPU config and DDP
       `gpu_config=dict(distributed=True,gpuid=0)`
   
       - distributed:
         - `True`: DDP will be used to train the model, at this time, you must use `dist_train.py` to start the experiment.
         - `False`: Single GPU Training will be used, at this time, you must use `train.py` to start the experiment
       - gpuid: this parameter only have effects in Single GPU Training, it specify the GPU to be used in the experiment.
   

# Configuration

## Introduction

Here we introduce another core component of **DeepMuon**: Configuration Files.

Configuration files are used to eliminate the modification of `train.py`,`dist_train.py`, which are core of training, we can make sure every researcher is smart enough to understand the logic of those core files, we want to provide a simple and direct experience of customizing training configurations, the DeepMuon training frame will automatically recognize the configurations specified within configuration files and apply them into training.

Configuration files are recommended to be stored in the folder `config`, before you create your configuration files, you must **STRICTLY** follow these rules:

> 1. **One Project, One Folder**
>
>    Here we know that there are several different projects are waiting for us. And during exploration of one project we have to try different configurations several times, so we need to make sure our management of projects and configurations of one project is tidy, direct, clear and even beautiful. Just as what I said before, researches are not only challenges to our intelligence but also to our taste of beauty and tidy.
>
> 3. **The edition of configuration files must follow the regulations given in the section `Configuration File Regulations`**
>
> 5. **DO NOT ADD** any executable console scripts into Dataset file, for instance: `print()`,`plt.show()`... all kinds of executable console scripts all forbidden in the file.

## Configuration File Regulations

1. **All these keywords must be presented in a configuration file:**

   - model
   - train_dataset
   - test_dataset
   - work_config
   - checkpoint_config
   - loss_fn
   - hyperpara
   - lr_config
   - gpu_config

2. **Regulations of keywords:**

   - Specify the model to be trained: 
   
     `model=dict(backbone='MLP3_3D_Direc')`

     - backbone: specify the model name, model will be picked up in the folder `models`
   
   - Specify the training dataset to be used to load the data, all dataset are stored in `dataset`: 
   
       `train_dataset=dict(backbone='HailingDataset_Direct',datapath='/home/dachuang2022/Yufeng/Hailing-Muon/data/1TeV/Hailing_1TeV_train_data.pkl')`
   
       - backbone: specify the name of Dataset class, Dataset will be loaded from folder `dataset`
       - datapath: specify the path of data, absolute path needed
   
   - Specify the testing dataset to be used to load the data, all dataset are stored in `dataset`: 
   
       `test_dataset=dict(backbone='HailingDataset_Direct',datapath='/home/dachuang2022/Yufeng/Hailing-Muon/data/1TeV/Hailing_1TeV_val_data.pkl')`
       
       - backbone: specify the name of Dataset class, Dataset will be loaded from folder `dataset`
       - datapath: specify the path of data, **absolute path needed**
       
   - Specify the work_dir to save the training log and checkpoints
       `work_config=dict(work_dir='/home/dachuang2022/Yufeng/Hailing-Muon/work_dir/1TeV/MLP3_3D',logfile='log.log')`
       
       - work_dir: the folder used to store the training logfile, tensorboard event foler `LOG`, and checkpoints. **Absolute path needed**
       - logfile: the name of the training logfile
       
   - Specify the checkpoint configuration
       `checkpoint_config=dict(load_from='',resume_from='',save_inter=10)`
       
       - load_from: the path of the pretrained `.pth` file to be loaded, model will be trained from epoch 0
       - resume_from: the path of pretrained `.pth` file to be used to resume the model training
       - save_inter: specify the checkpoint saving frequency
       
   - Specify the customized loss function to be used, if no customized loss function specified, nn.MSELoss() will be used
     
       If `loss_fn=None` specified, `nn.MSELoss()` will be used to train the model, otherwise `loss_fn=dict(backbone='')`
       
       - backbone: the name of the loss function created in file `Airloss.py`
       
   - Specify the Hyperparameters to be used
       `hyperpara=dict(epochs=10,batch_size=400,inputshape=[1,10,10,40,3])`
       
       - epochs: the training epochs
       - batch_size: the training batch_size
       - inputshape: the shape of the model input data, first element is the batch_size(here is 1), and the left elements are actual data shape
       
   - Specify the lr as well as its config, the lr will be optimized using `torch.optim.lr_scheduler.ReduceLROnPlateau()`
       `lr_config=dict(init=0.0005,patience=100)`
       
       - init: the initial learning rate
       - patience: the patience epochs used to judge the learning rate dacay status
       
   - Specify the GPU config and DDP
       `gpu_config=dict(distributed=True,gpuid=0)`
       
       - distributed:
         - `True`: DDP will be used to train the model, at this time, you must use `dist_train.py` to start the experiment.
         - `False`: Single GPU Training will be used, at this time, you must use `train.py` to start the experiment
       - gpuid: this parameter only have effects in Single GPU Training, it specify the GPU to be used in the experiment.
